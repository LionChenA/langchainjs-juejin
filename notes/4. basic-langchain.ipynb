{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那 LCEL 有什么优势呢？\n",
    "LCEL 从底层设计的目标就是支持 从原型到生产 完整流程不需要修改任何代码，也就是我们在写的任何原型代码不需要太多的改变就能支持生产级别的各种特性（比如并行、steaming 等），具体来说会有这些优势：\n",
    "\n",
    "- 并行，只要是整个 chain 中有可以并行的步骤就会自动的并行，来减少使用时的延迟。\n",
    "- 自动的重试和 fallback。大部分 chain 的组成部分都有自动的重试（比如因为网络原因的失败）和回退机制，来解决很多请求的出错问题。 而不需要我们去写代码 cover 这些问题。\n",
    "- 对 chain 中间结果的访问，在旧的写法中很难访问中间的结果，而 LCEL 中可以方便的通过访问中间结果来进行调试和记录。\n",
    "- LCEL 会自动支持 LangSimith 进行可视化和记录。这是 langchain 官方推出的记录工具，可以记录一条 chian 运行过程中的大部分信息，来方便调试 LLM 找到是哪些中间环节的导致了最终结果较差。这部分我们会在后续的章节中涉及到。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# invoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  \"id\": \"chatcmpl-B7P4dGluhVkUMPGllFQrEgaxFyNXV\",\n",
       "  \"content\": \"Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {\n",
       "    \"tokenUsage\": {\n",
       "      \"promptTokens\": 11,\n",
       "      \"completionTokens\": 18,\n",
       "      \"totalTokens\": 29\n",
       "    },\n",
       "    \"finish_reason\": \"stop\",\n",
       "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
       "    \"usage\": {\n",
       "      \"completion_tokens\": 18,\n",
       "      \"completion_tokens_details\": {\n",
       "        \"accepted_prediction_tokens\": 0,\n",
       "        \"audio_tokens\": 0,\n",
       "        \"reasoning_tokens\": 0,\n",
       "        \"rejected_prediction_tokens\": 0\n",
       "      },\n",
       "      \"prompt_tokens\": 11,\n",
       "      \"prompt_tokens_details\": {\n",
       "        \"audio_tokens\": 0,\n",
       "        \"cached_tokens\": 0\n",
       "      },\n",
       "      \"total_tokens\": 29\n",
       "    },\n",
       "    \"system_fingerprint\": \"fp_b705f0c291\"\n",
       "  },\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": {\n",
       "    \"output_tokens\": 18,\n",
       "    \"input_tokens\": 11,\n",
       "    \"total_tokens\": 29,\n",
       "    \"input_token_details\": {\n",
       "      \"audio\": 0,\n",
       "      \"cache_read\": 0\n",
       "    },\n",
       "    \"output_token_details\": {\n",
       "      \"audio\": 0,\n",
       "      \"reasoning\": 0\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "import { HumanMessage } from \"@langchain/core/messages\"\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://yunwu.ai/v1\",\n",
    "  },\n",
    "  modelName: \"gpt-4o-mini\",\n",
    "})\n",
    "\n",
    "await model.invoke([\n",
    "  new HumanMessage(\"Tell me a joke\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便展示，我们会加入一个简单的 `StringOutputParser` 来处理输出，你可以简单的理解为将 OpenAI 返回的复杂对象提取出最核心的字符串，更详细的 `OutputParser` 相关介绍会在后续章节中展开。组成一个最基础的 Chain 来演示， `Runnable` 中各个调用方式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { StringOutputParser } from \"@langchain/core/output_parsers\"\n",
    "\n",
    "const chatModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://yunwu.ai/v1\",\n",
    "  },\n",
    "  modelName: \"gpt-4o-mini\",\n",
    "})\n",
    "\n",
    "const outputPrase = new StringOutputParser()\n",
    "\n",
    "const simpleChain = chatModel.pipe(outputPrase)\n",
    "\n",
    "await simpleChain.invoke([\n",
    "  new HumanMessage(\"Tell me a joke\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 StringOutputParser，我们获取了 AIMessage 中 content 的值。在 LCEL 中，使用 `.pipe()` 方法来组装多个 `Runnable` 对象形成完整的 Chain，可以看到我们是用对单个模块同样的 `invoke` 方法去调用整个 chain。 因为无论是单个模块还是由模块组装而成的多个 chain 都是 `Runnable`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们尝试对这个基础的 Chain 进行批量调用，用起来也非常简单\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  \u001b[32m\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\"\u001b[39m,\n",
       "  \u001b[32m\"Hello! I’m an AI language model created by OpenAI. I'm here to assist you with information, answer questions, and engage in conversation on a variety of topics. How can I help you today?\"\u001b[39m\n",
       "]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await simpleChain.batch([\n",
    "  [new HumanMessage(\"Tell me a joke\")],\n",
    "  [new HumanMessage(\"Hi, Who are you?\")],\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为 LLM 的很多调用都是一段一段的返回的，如果等到完整地内容再返回给用户，就会让用户等待比较久，影响用户的体验。而 LCEL 开箱就支持 steaming，即所谓的流式传输，我们依旧使用我们定义的基础 Chain，就可以直接获得 streaming 的能力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " scare\n",
      "crow\n",
      " win\n",
      " an\n",
      " award\n",
      "?\n",
      " \n",
      "\n",
      "\n",
      "Because\n",
      " he\n",
      " was\n",
      " outstanding\n",
      " in\n",
      " his\n",
      " field\n",
      "!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const stream = await simpleChain.stream([\n",
    "  new HumanMessage(\"Tell me a joke\"),\n",
    "])\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "streamLog 的使用较少，他会在每次返回 chunk 的时候，返回完整的对象，我们不深入介绍，感兴趣的可以运行下述代码观察其每个 chunk 的返回值，并根据自己需要去使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"replace\"\u001b[39m,\n",
      "      path: \u001b[32m\"\"\u001b[39m,\n",
      "      value: {\n",
      "        id: \u001b[32m\"1083ced0-fd36-4590-83d4-84360ba2d9ea\"\u001b[39m,\n",
      "        name: \u001b[32m\"RunnableSequence\"\u001b[39m,\n",
      "        type: \u001b[32m\"chain\"\u001b[39m,\n",
      "        streamed_output: [],\n",
      "        final_output: \u001b[90mundefined\u001b[39m,\n",
      "        logs: {}\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI\"\u001b[39m,\n",
      "      value: {\n",
      "        id: \u001b[32m\"bbad8453-0d7a-4c0f-a119-0a838d129bfb\"\u001b[39m,\n",
      "        name: \u001b[32m\"ChatOpenAI\"\u001b[39m,\n",
      "        type: \u001b[32m\"llm\"\u001b[39m,\n",
      "        tags: [ \u001b[32m\"seq:step:1\"\u001b[39m ],\n",
      "        metadata: {\n",
      "          ls_provider: \u001b[32m\"openai\"\u001b[39m,\n",
      "          ls_model_name: \u001b[32m\"gpt-4o-mini\"\u001b[39m,\n",
      "          ls_model_type: \u001b[32m\"chat\"\u001b[39m,\n",
      "          ls_temperature: \u001b[90mundefined\u001b[39m,\n",
      "          ls_max_tokens: \u001b[90mundefined\u001b[39m,\n",
      "          ls_stop: \u001b[90mundefined\u001b[39m\n",
      "        },\n",
      "        start_time: \u001b[32m\"2025-03-04T16:20:47.178Z\"\u001b[39m,\n",
      "        streamed_output: [],\n",
      "        streamed_output_str: [],\n",
      "        final_output: \u001b[90mundefined\u001b[39m,\n",
      "        end_time: \u001b[90mundefined\u001b[39m\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser\"\u001b[39m,\n",
      "      value: {\n",
      "        id: \u001b[32m\"f7ab57a3-15cf-43a8-b71a-70d4bd7024c3\"\u001b[39m,\n",
      "        name: \u001b[32m\"StrOutputParser\"\u001b[39m,\n",
      "        type: \u001b[32m\"parser\"\u001b[39m,\n",
      "        tags: [ \u001b[32m\"seq:step:2\"\u001b[39m ],\n",
      "        metadata: {},\n",
      "        start_time: \u001b[32m\"2025-03-04T16:20:49.865Z\"\u001b[39m,\n",
      "        streamed_output: [],\n",
      "        streamed_output_str: [],\n",
      "        final_output: \u001b[90mundefined\u001b[39m,\n",
      "        end_time: \u001b[90mundefined\u001b[39m\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"Why\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"Why\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"Why\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"Why\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"Why\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" did\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" did\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" did\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" did\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" did\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" the\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" the\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" the\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" the\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" the\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" scare\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" scare\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" scare\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" scare\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" scare\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"crow\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"crow\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"crow\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"crow\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"crow\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" win\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" win\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" win\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" win\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" win\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" an\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" an\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" an\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" an\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" an\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" award\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" award\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" award\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" award\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" award\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"?\\n\\n\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"?\\n\\n\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"?\\n\\n\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"?\\n\\n\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"?\\n\\n\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"Because\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"Because\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"Because\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"Because\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"Because\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" he\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" he\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" he\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" he\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" he\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" was\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" was\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" was\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" was\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" was\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" outstanding\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" outstanding\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" outstanding\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" outstanding\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" outstanding\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" in\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" in\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" in\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" in\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" in\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" his\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" his\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" his\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" his\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" his\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" field\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\" field\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\" field\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\" field\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \" field\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"!\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"!\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"!\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"!\"\u001b[39m,\n",
      "        generationInfo: { prompt: \u001b[33m0\u001b[39m, completion: \u001b[33m0\u001b[39m },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"!\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output_str/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"\"\u001b[39m\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/streamed_output/-\"\u001b[39m,\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \u001b[32m\"\"\u001b[39m,\n",
      "        generationInfo: {\n",
      "          prompt: \u001b[33m0\u001b[39m,\n",
      "          completion: \u001b[33m0\u001b[39m,\n",
      "          finish_reason: \u001b[32m\"stop\"\u001b[39m,\n",
      "          system_fingerprint: \u001b[32m\"fp_b705f0c291\"\u001b[39m,\n",
      "          model_name: \u001b[32m\"gpt-4o-mini-2024-07-18\"\u001b[39m\n",
      "        },\n",
      "        message: AIMessageChunk {\n",
      "          \"id\": \"chatcmpl-B7PIfz45R7ta8QoZbk5KW1rCrG6MO\",\n",
      "          \"content\": \"\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {\n",
      "            \"prompt\": 0,\n",
      "            \"completion\": 0,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"system_fingerprint\": \"fp_b705f0c291\",\n",
      "            \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "            \"usage\": {}\n",
      "          },\n",
      "          \"tool_calls\": [],\n",
      "          \"tool_call_chunks\": [],\n",
      "          \"invalid_tool_calls\": []\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/streamed_output/-\"\u001b[39m,\n",
      "      value: \u001b[32m\"\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \u001b[32m\"add\"\u001b[39m, path: \u001b[32m\"/streamed_output/-\"\u001b[39m, value: \u001b[32m\"\"\u001b[39m } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/final_output\"\u001b[39m,\n",
      "      value: { generations: [ \u001b[36m[Array]\u001b[39m ], llmOutput: { tokenUsage: \u001b[36m[Object]\u001b[39m } }\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/ChatOpenAI/end_time\"\u001b[39m,\n",
      "      value: \u001b[32m\"2025-03-04T16:20:49.935Z\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/final_output\"\u001b[39m,\n",
      "      value: {\n",
      "        output: \u001b[32m\"Why did the scarecrow win an award?\\n\"\u001b[39m +\n",
      "          \u001b[32m\"\\n\"\u001b[39m +\n",
      "          \u001b[32m\"Because he was outstanding in his field!\"\u001b[39m\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      op: \u001b[32m\"add\"\u001b[39m,\n",
      "      path: \u001b[32m\"/logs/StrOutputParser/end_time\"\u001b[39m,\n",
      "      value: \u001b[32m\"2025-03-04T16:20:49.935Z\"\u001b[39m\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \u001b[32m\"replace\"\u001b[39m,\n",
      "      path: \u001b[32m\"/final_output\"\u001b[39m,\n",
      "      value: {\n",
      "        output: \u001b[32m\"Why did the scarecrow win an award?\\n\"\u001b[39m +\n",
      "          \u001b[32m\"\\n\"\u001b[39m +\n",
      "          \u001b[32m\"Because he was outstanding in his field!\"\u001b[39m\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const stream = await simpleChain.streamLog([\n",
    "  new HumanMessage(\"Tell me a joke\"),\n",
    "])\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fallback\n",
    "\n",
    "`withFallbacks` 是任何 runnable 都有的一个函数，可以给当前 runnable 对象添加 fallback 然后生成一个带 fallback 的 `RunnableWithFallbacks` 对象，这适合我们将自己的 fallback 逻辑增加到 LCEL 中。\n",
    "\n",
    "例如，我们创建一个一定会失败的 llm ：\n",
    "\n",
    "> 注意这里要多等一会（1-2min）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "Error: Connection error.",
      "    at OpenAI.makeRequest (file:///Users/lion/Developer/llm/langchainjs-juejin/node_modules/.deno/openai@4.86.1/node_modules/openai/core.mjs:322:19)",
      "    at eventLoopTick (ext:core/01_core.js:177:7)",
      "    at async file:///Users/lion/Developer/llm/langchainjs-juejin/node_modules/.deno/@langchain+openai@0.4.4/node_modules/@langchain/openai/dist/chat_models.js:1523:29",
      "    at async RetryOperation._fn (file:///Users/lion/Developer/llm/langchainjs-juejin/node_modules/.deno/p-retry@4.6.2/node_modules/p-retry/index.js:50:12)"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "\n",
    "const fakeLLM = new ChatOpenAI({\n",
    "  openAIApiKey: \"123\",\n",
    "  maxRetries: 0,\n",
    "})\n",
    "\n",
    "await fakeLLM.invoke(\"你好\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而正确的呢？下述代码也是用了很久返回了正确的内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  \"id\": \"chatcmpl-B7PZj7DvVH1C3igNV1SchHGZIOckJ\",\n",
       "  \"content\": \"你好！有什么我可以帮助你的吗？\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {\n",
       "    \"tokenUsage\": {\n",
       "      \"promptTokens\": 8,\n",
       "      \"completionTokens\": 9,\n",
       "      \"totalTokens\": 17\n",
       "    },\n",
       "    \"finish_reason\": \"stop\",\n",
       "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
       "    \"usage\": {\n",
       "      \"completion_tokens\": 9,\n",
       "      \"completion_tokens_details\": {\n",
       "        \"accepted_prediction_tokens\": 0,\n",
       "        \"audio_tokens\": 0,\n",
       "        \"reasoning_tokens\": 0,\n",
       "        \"rejected_prediction_tokens\": 0\n",
       "      },\n",
       "      \"prompt_tokens\": 8,\n",
       "      \"prompt_tokens_details\": {\n",
       "        \"audio_tokens\": 0,\n",
       "        \"cached_tokens\": 0\n",
       "      },\n",
       "      \"total_tokens\": 17\n",
       "    },\n",
       "    \"system_fingerprint\": \"fp_b705f0c291\"\n",
       "  },\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": {\n",
       "    \"output_tokens\": 9,\n",
       "    \"input_tokens\": 8,\n",
       "    \"total_tokens\": 17,\n",
       "    \"input_token_details\": {\n",
       "      \"audio\": 0,\n",
       "      \"cache_read\": 0\n",
       "    },\n",
       "    \"output_token_details\": {\n",
       "      \"audio\": 0,\n",
       "      \"reasoning\": 0\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const realLLM = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://yunwu.ai/v1\",\n",
    "  },\n",
    "  modelName: \"gpt-4o-mini\",\n",
    "})\n",
    "\n",
    "const llmWithFallback = fakeLLM.withFallbacks({\n",
    "  fallbacks: [realLLM],\n",
    "})\n",
    "\n",
    "await llmWithFallback.invoke(\"你好\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "因为无论是 llm model 或者其他的模块，还是整个 chain 都是 runnable 对象，所以我们可以给整个 LCEL 流程中的任意环节去增加 fallback，来避免一个环节出问题卡住剩下环境的运行。\n",
    "\n",
    "当然，我们也可以给整个 chain 增加 fallback，例如一个复杂但输出高质量的结果的 chain 可以设置一个非常简单的 chain 作为 fallback，可以在极端环境下保证至少有输出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "That's All!  \n",
    "这就是 langchain.js 基础，如果你在 LCEL 之前学习过 langchain，你会发现 LCEL 极大的降低了 langchain 的使用难度，并且为使用 chain 提供了开箱即用的生产级能力支持。其最大的魅力就是进一步强化了模块化，可以方便的复用各种 chain 来组合成更复杂的 chain。\n",
    "\n",
    "所以我认为，在当前时间点可以抛弃之前的旧写法，全面拥抱 LCEL。 在掌握了基础用法后，我们就可以探索更多 langchain 的应用了。至于更多 LCEL 的高级用法，我们会随着实战逐步引入和讲解。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
